{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (65931, 32, 32), (65931, 1))\n",
      "('Validation set', (7326, 32, 32), (7326, 1))\n",
      "('Test set', (26032, 32, 32), (26032, 1))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "pickle_file = 'SVHN_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_data = save['train_data']\n",
    "    test_data = save['test_data']\n",
    "    val_data = save['val_data']\n",
    "    train_label = save['train_label']\n",
    "    test_label = save['test_label']\n",
    "    val_label = save['val_label']\n",
    "    del save  # hint to help gc free up memory\n",
    "    \n",
    "    print('Training set', train_data.shape, train_label.shape)\n",
    "    print('Validation set', val_data.shape, val_label.shape)\n",
    "    print('Test set', test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.hist(train_label[:,0], bins = 10)\n",
    "plt.xlabel(\"Numbers in the label\") \n",
    "plt.ylabel(\"Number frequency\")\n",
    "plt.title(\"Train Label\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(211)\n",
    "plt.hist(test_label[:,0])\n",
    "plt.xlabel(\"Numbers in the label\") \n",
    "plt.ylabel(\"Number frequency\")\n",
    "plt.title(\"Test Label\")\n",
    "\n",
    "plt.figure(3)\n",
    "plt.subplot(211)\n",
    "plt.hist(val_label[:,0])\n",
    "plt.xlabel(\"Numbers in the label\") \n",
    "plt.ylabel(\"Number frequency\")\n",
    "plt.title(\"Validation Label\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (65931, 32, 32, 1), (65931, 11))\n",
      "('Validation set', (7326, 32, 32, 1), (7326, 11))\n",
      "('Test set', (26032, 32, 32, 1), (26032, 11))\n"
     ]
    }
   ],
   "source": [
    "# Reformat into a TensorFlow-friendly shape\n",
    "image_size = 32\n",
    "num_labels = 11\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformat(data, labels):\n",
    "    data = data.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return data, labels\n",
    "\n",
    "train_data, train_label = reformat(train_data, train_label[:,0])\n",
    "val_data, val_label = reformat(val_data, val_label[:,0])\n",
    "test_data, test_label = reformat(test_data, test_label[:,0])\n",
    "\n",
    "print('Training set', train_data.shape, train_label.shape)\n",
    "print('Validation set', val_data.shape, val_label.shape)\n",
    "print('Test set', test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "output1 = 64\n",
    "output2 = 32\n",
    "beta = 0.01 # beta parameter for L2 loss\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(None, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    tf_valid_data = tf.constant(val_data)\n",
    "    tf_test_data = tf.constant(test_data)\n",
    "    \n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def bias_variable(shape, init):\n",
    "        initial = tf.constant(init, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(x, W, padding):\n",
    "        # stride [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding = padding)\n",
    "    \n",
    "    def max_pool_2x2(x):\n",
    "        # stride [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    \n",
    "    #Layer variables:\n",
    "    \n",
    "    # conv layer 1\n",
    "    conv1_W = weight_variable([patch_size, patch_size, num_channels, depth1]) # [5, 5, 1, 16]\n",
    "    conv1_b = bias_variable([1, depth1], 0.1)\n",
    "    \n",
    "    # conv layer 2\n",
    "    conv2_W = weight_variable([patch_size, patch_size, depth1, depth2]) #[5, 5, 16, 32]\n",
    "    conv2_b = bias_variable([1, depth2], 1.0)\n",
    "    \n",
    "    # conv layer 3\n",
    "    conv3_W = weight_variable([patch_size, patch_size, depth2, depth3]) #[5, 5, 32, 64]\n",
    "    conv3_b = bias_variable([1, depth3], 1.0)\n",
    "    \n",
    "    # fc layer 1\n",
    "    fc1_W = weight_variable([output1, output2]) # 64, 32\n",
    "    fc1_b = bias_variable([1, output2], 1.0)\n",
    "    \n",
    "    # fc layer 2\n",
    "    fc2_W = weight_variable([output2, num_labels])\n",
    "    fc2_b = bias_variable([1, num_labels], 1.0) # 32, 11\n",
    "    \n",
    "    def model(data, keep_prob):  \n",
    "        \n",
    "        # conv layer 1\n",
    "        conv1_h = tf.nn.relu(conv2d(data, conv1_W, 'VALID') + conv1_b) # 28 * 28* 16\n",
    "        # Local Response Normalization\n",
    "        norm1 = tf.nn.local_response_normalization(conv1_h)\n",
    "        # Max Pooling 1\n",
    "        pool1 = max_pool_2x2(norm1) # 14 * 14 * 16\n",
    "        \n",
    "        # conv layer 2\n",
    "        conv2_h = tf.nn.relu(conv2d(pool1, conv2_W, 'VALID') + conv2_b) # 10 * 10 * 32\n",
    "        # Local Response Normalization\n",
    "        norm2 = tf.nn.local_response_normalization(conv2_h)\n",
    "        #Max Pooling 2\n",
    "        pool2 = max_pool_2x2(norm2) # 5 * 5 * 32\n",
    "        \n",
    "        # conv layer 3\n",
    "        conv3_h = tf.nn.relu(conv2d(pool2, conv3_W, 'VALID') + conv3_b) # 1 * 1 * 64\n",
    "        # Drop out\n",
    "        conv3_drop = tf.nn.dropout(conv3_h, keep_prob)\n",
    "        \n",
    "        # fc layer 1\n",
    "        pool2_flat = tf.reshape(conv3_drop, [-1, 1 * 1 * 64]) # reshape pooled picture\n",
    "        fc1_h = tf.nn.relu(tf.matmul(pool2_flat, fc1_W) + fc1_b) # 64, 32\n",
    "        \n",
    "        # fc layer 2\n",
    "        fc2_h = tf.matmul(fc1_h, fc2_W) + fc2_b # 32, 11\n",
    "        \n",
    "        return fc2_h\n",
    "  \n",
    "    # Training computation\n",
    "    \n",
    "    out_biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "    \n",
    "    pred_result = model(tf_train_data, 0.7)\n",
    "    \n",
    "    # Add L2 regulation into entropy loss\n",
    "    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred_result, tf_train_labels) + \n",
    "             beta * tf.nn.l2_loss(conv1_b) +\n",
    "             beta * tf.nn.l2_loss(conv2_b) +\n",
    "             beta * tf.nn.l2_loss(conv3_b) +\n",
    "            beta * tf.nn.l2_loss(fc1_W) + beta * tf.nn.l2_loss(fc1_b) +\n",
    "            beta * tf.nn.l2_loss(fc2_W) + beta * tf.nn.l2_loss(fc2_b)))\n",
    "    \n",
    "    # Tensorboard Record loss\n",
    "    tf.scalar_summary('loss', loss)\n",
    "     \n",
    "    # Optimizer\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, batch, 3000, 0.95, staircase=True) \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step = batch)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data\n",
    "    train_prediction = tf.nn.softmax(model(tf_train_data, 1.0))\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_data, 1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_data, 1.0))\n",
    "    \n",
    "    # Save trained model weights for future use\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.427294\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 9.6%\n",
      "Minibatch loss at step 500: 1.096054\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1000: 0.655034\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1500: 0.572835\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 2000: 0.620293\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2500: 0.538856\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3000: 0.351000\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3500: 0.461504\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 4000: 0.805097\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 4500: 0.401264\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 5000: 0.873088\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 5500: 0.437264\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 6000: 0.562095\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 6500: 0.458213\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 7000: 0.460669\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 7500: 0.421750\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8000: 0.462175\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8500: 0.589105\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 9000: 0.309114\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9500: 0.262738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 10000: 0.391742\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 10500: 0.525685\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 11000: 0.598564\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 11500: 0.337425\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 12000: 0.443796\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 12500: 0.382944\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 13000: 0.494539\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 13500: 0.344092\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 14000: 0.349963\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 14500: 0.411362\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 15000: 0.291253\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.4%\n",
      "Test accuracy: 89.9%\n",
      "Trained Model weights are saved in: SVHN_weight.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    # Tensorboard Summary writer\n",
    "    merged = tf.merge_all_summaries()\n",
    "    train_writer = tf.train.SummaryWriter(\"result/\", session.graph)\n",
    "    \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        # Get 64 batch images\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        # feed dictionary \n",
    "        feed_dic = {tf_train_data : batch_data, tf_train_labels : batch_labels}\n",
    "        # Run the graph to get all the variables\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dic)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            \n",
    "            # Record loss and accuracy every 500 steps  \n",
    "            train_result = session.run(merged, feed_dict = feed_dic)\n",
    "            train_writer.add_summary(train_result, step)\n",
    "            \n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), val_label))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_label))\n",
    "    \n",
    "    save_weight = saver.save(session, 'SVHN_weight.ckpt')\n",
    "    print('Trained Model weights are saved in: %s' % save_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
